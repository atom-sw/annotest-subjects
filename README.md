# 1. aNNoTest subjects

The current repository contains 62 bugs collected in [Islam et al. (2019)](https://dl.acm.org/doi/10.1145/3338906.3338955),
which provided these bugs as links to
line numbers within different commits, each representing one bug.

Proposing aNNoTest, an annotation-based test generation tool 
for neural network programs, we managed to generate failing 
test cases that reproduce these bugs. 
The details of aNNoTest and how we generated
these failing test cases can be found in the 
following paper:

[An annotation-based approach for 
finding bugs in neural network programs, by
Mohammad Rezaalipour and Carlo A. Furia.](#13-citations).

## 1.1 Bugs information

File [bugs_info.csv](bugs_info.csv) contains information about these
62 bugs in this repository. For each bug, this file includes 
its project name, the framework/library used to develop it, and a link
to commit line numbers representing the bug.

For each bug, we added the following items to make it easy for users to reproduce these bugs:

- aN annotations (within the bug's source code) required by aNNoTest to generate tests.

- Comment `repo_bug` within the source code, indicating bug's location.

- File `requirements.txt` containing project dependencies.

- Script `make_env.sh` that creates a conda environment for the bug and installs all the bug's dependencies.

- Script `make_data.sh` that downloads the data required by the bug. Only bugs that need such data have this script.

- The failing test we generated for the bug, using aNNoTest. We refer to this test as aNNoTest test. Following the instructions in the current repository, users can generate this test themselves too, using aNNoTest.

- File `annotest_test_name.txt` including the name of aNNoTest test that can reproduce the bug.

- A failing test in Pytest format that we manually wrote from `aNNoTest` test to make it easy for users to understand what the `aNNoTest` test does.

- Script `run_annotest_failing_test.sh` that activates the bug's conda environment and runs the `aNNoTest` test.

- Script `run_manual_failing_test.sh` that activates the bug's conda environment and runs the manually produced failing test.

- File `expected_error_message.txt`, including the error message aNNoTest test and the manually produced test must report.
 

## 1.2 Reproducing bugs

To reproduce the bugs, follow the instructions below:

1. Install Anaconda 3 on your machine, following
the instructions on 
[Anaconda's documentation 
website](https://docs.anaconda.com/free/anaconda/install/index.html).


2. Set the enviroment varaible `ANACONDA3_DIRECTORY` to
point to `anaconda3` directory on your machine.
For instance, if anaconda3 is in your home directory
(i.e., `~/anaconda3`), run the following command
or put it in your `~/.profile` file.

```
export ANACONDA3_DIRECTORY=~/anaconda3
```

3. Clone the repository, and cd to the repository directory on your machine.

```
git clone git@github.com:atom-sw/annotest-subjects.git
```

```
cd annotest-subjects
```

4. For each bug, there is a directory in the repo.
First, `cd` to that directory (e.g. `densenet_b1`), and then, make the
bash scripts in that directory executable.

```
cd densenet_b1
```

```
chmod +x *.sh
```

5. Run the following bash script to make a conda environment for the bug and install all the bug's dependencies in that environment.

```
./make_env.sh
```

6. Some bugs require some datasets or files.
for such bugs, there is a bash script named `make_data.sh`,
running which downloads these dependencies and puts them
in directory `~/annotest_subjects_data` on your machine.

```
./make_data.sh
```

7. To reproduce the bug, you must install aNNoTest inside the
conda environment created by running script `make_env.sh`.
The name of the conda environment is the same as the bug 
id (i.e., the bug's directory name), which is `densenet_b1` in our example.
So, first activate environment the bug's environment, and then
install aNNoTest.

```
conda activate densenet_b1
```

```
pip install annotest
```

8. Run aNNoTest to generate the failing test.
All of the projects in this repository are already
annotated. So, you do not need to annotate them, yourself.

```
annotest .
```

9. Run the following bash script to execute the
Hypothesis test generated by aNNoTest. This bash script
simply activates the bug's conda environment and runs
the Hypothesis test generated by aNNoTest. You can find the
name of these tests in files named `annotest_test_name.txt` (e.g., [annotest_test_name.txt](densenet_b1/annotest_test_name.txt)).

```
./run_annotest_failing_test.sh
```

The error message must be the same as the one in file `expected_error_message.txt` in the bug's directory.

10. For every bug, we have included a Pytest
failing test that also reproduces the bug. We wrote
these tests manually using the Hypothesis tests aNNoTest
generated. Execute the following command to run the manual
failing test.

```
./run_manual_failing_test.sh
```

The error message must be the same as the one in file `expected_error_message.txt` in the bug's directory.

## 1.3 Citations

[aNNoTest's Journal 
Paper:](https://doi.org/10.1016/j.jss.2023.111669)

```
@article{Rezaalipour:2023,
title = {An annotation-based approach for finding bugs in neural network programs},
journal = {Journal of Systems and Software},
volume = {201},
pages = {111669},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111669},
url = {https://www.sciencedirect.com/science/article/pii/S016412122300064X},
author = {Mohammad Rezaalipour and Carlo A. Furia},
keywords = {Test generation, Neural networks, Debugging, Python},
}
```

# 2. Mirrors

The current repository is a public mirror of
our internal private repository.
We have two public mirrors, which are as follows:

- https://github.com/atom-sw/annotest-subjects
- https://github.com/mohrez86/annotest-subjects
