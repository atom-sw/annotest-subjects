{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Regularization in Neural Network</center>\n",
    "<center>Shan-Hung Wu & DataLab<br/>Fall 2016</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# inline plotting instead of popping out\n",
    "%matplotlib inline\n",
    "\n",
    "# load utility classes/functions that has been taught in previous labs\n",
    "# e.g., plot_decision_regions()\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "sys.path.append(module_path)\n",
    "from lib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural network with a large number of parameters is a powerful machine learning system, but overfitting is a serious issues in such networks. Deep neural network contains many non-linear hidden units and thus they can express very complicated relationships between their inputs and outputs. With limited training data, many of these relationships will be the result of sampling noise, so these relationships exist only on the training dataset, but not on the testing dataset. Large networks are slow, so it's hard for us to deal with the overfitting issues by training several networks with different architecture and combine their outputs.\n",
    "In this lab, we are going to talk about regularization in neural network.\n",
    "We will introduce some common regularization methods in deep neural network today, which is \n",
    "* Dropout\n",
    "* Maxout\n",
    "* Weight decay\n",
    "* Adding noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the CIFAR-10 dataset today for our experiment. [CIFAR-10]() is a object recognition dataset of 10 class. The CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n",
    "\n",
    "Here are the classes in the dataset, as well as 10 random images from each:\n",
    "<img src=\"fig-cifar-10.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocess the CIFAR-10 dataset\n",
    "\n",
    "## Load data\n",
    "set the path for storing the dataset on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data/CIFAR-10/\"\n",
    "data_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and some constants for processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the width and height of out image\n",
    "img_size = 32\n",
    "# number of channels: red, green and blue\n",
    "img_channels = 3\n",
    "# length of the image after we flatten the image into a 1-dim array\n",
    "img_size_flat = img_size * img_size * img_channels\n",
    "# number of classes\n",
    "nb_classes = 10\n",
    "# number of files in the training dataset\n",
    "nb_files_train = 5\n",
    "# number of images for each batch-file in the training-set.\n",
    "images_per_file = 10000\n",
    "# number of all the images in the training dataset\n",
    "nb_images_train = nb_files_train * images_per_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we download and unzip the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# filename for saving the file downloaded from the internet.\n",
    "filename = data_url.split('/')[-1]\n",
    "file_path = os.path.join(data_path, filename)\n",
    "\n",
    "# Check if the file already exists.\n",
    "# If it exists then we assume it has also been extracted,\n",
    "# otherwise we need to download and extract it now.\n",
    "if not os.path.exists(file_path):\n",
    "    # Check if the download directory exists, otherwise create it.\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    # Download the file from the internet.\n",
    "    file_path, _ = urllib.request.urlretrieve(url=data_url,\n",
    "                                              filename=file_path,\n",
    "                                              reporthook=_print_download_progress)\n",
    "\n",
    "    print()\n",
    "    print(\"Download finished. Extracting files.\")\n",
    "\n",
    "    if file_path.endswith(\".zip\"):\n",
    "        # Unpack the zip-file.\n",
    "        zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
    "    elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "        # Unpack the tar-ball.\n",
    "        tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
    "\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"Data has already been downloaded and unpacked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the classes name in the CIFAR-10 dataset from the metafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/CIFAR-10/cifar-10-batches-py/batches.meta\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "# Create full path for the file.\n",
    "file_path = os.path.join(data_path, \"cifar-10-batches-py/\", \"batches.meta\")\n",
    "\n",
    "print(\"Loading data: \" + file_path)\n",
    "\n",
    "with open(file_path, mode='rb') as file:\n",
    "    # In Python 3.X it is important to set the encoding,\n",
    "    # otherwise an exception is raised here.\n",
    "    data = pickle.load(file, encoding='bytes')\n",
    "\n",
    "raw = data[b'label_names']\n",
    "\n",
    "# Convert from binary strings.\n",
    "class_names = [x.decode('utf-8') for x in raw]\n",
    "# class_names a list with the names. Example: names[3] is the name associated with class-number 3.\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: Unable to get the number of gpus available: CUDA driver version is insufficient for CUDA runtime version)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    Load a pickled data-file from the CIFAR-10 data-set\n",
    "    and return the converted images (see above) and the class-number\n",
    "    for each image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create full path for the file.\n",
    "    file_path = os.path.join(data_path, \"cifar-10-batches-py/\", file_name)\n",
    "\n",
    "    print(\"Loading data: \" + file_path)\n",
    "\n",
    "    with open(file_path, mode='rb') as file:\n",
    "        # In Python 3.X it is important to set the encoding,\n",
    "        # otherwise an exception is raised here.\n",
    "        data = pickle.load(file, encoding='bytes')\n",
    "\n",
    "    # Get the raw images.\n",
    "    raw_images = data[b'data']\n",
    "\n",
    "    # Get the class-numbers for each image. Convert to numpy-array.\n",
    "    cls = np.array(data[b'labels'])\n",
    "\n",
    "    # Convert the images.\n",
    "    \"\"\"\n",
    "    Convert images from the CIFAR-10 format and\n",
    "    return a 4-dim array with shape: [image_number, channel, height, width]\n",
    "    where the pixels are floats between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the raw images from the data-files to floating-points.\n",
    "    raw_float = np.array(raw_images, dtype=float) / 255.0\n",
    "\n",
    "    # Reshape the array to a 4-dim array with shape: [image_number, channel, height, width] where the pixels are floats between 0.0 and 1.0.\n",
    "    images = raw_float.reshape([-1, img_channels, img_size, img_size])\n",
    "\n",
    "    return images, cls\n",
    "\n",
    "def load_training_data():\n",
    "    \"\"\"\n",
    "    Load all the training-data for the CIFAR-10 data-set.\n",
    "    The data-set is split into 5 data-files which are merged here.\n",
    "    Returns the images, class-numbers and one-hot encoded class-labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pre-allocate the arrays for the images and class-numbers for efficiency.\n",
    "    images = np.zeros(shape=[nb_images_train, img_channels, img_size, img_size], dtype=float)\n",
    "    cls = np.zeros(shape=[nb_images_train], dtype=int)\n",
    "\n",
    "    # Begin-index for the current batch.\n",
    "    begin = 0\n",
    "\n",
    "    # For each data-file.\n",
    "    for i in range(nb_files_train):\n",
    "        # Load the images and class-numbers from the data-file.\n",
    "        images_batch, cls_batch = load_data(file_name=\"data_batch_\" + str(i + 1))\n",
    "\n",
    "        # Number of images in this batch.\n",
    "        num_images = len(images_batch)\n",
    "\n",
    "        # End-index for the current batch.\n",
    "        end = begin + num_images\n",
    "\n",
    "        # Store the images into the array.\n",
    "        images[begin:end, :] = images_batch\n",
    "\n",
    "        # Store the class-numbers into the array.\n",
    "        cls[begin:end] = cls_batch\n",
    "\n",
    "        # The begin-index for the next batch is the current end-index.\n",
    "        begin = end\n",
    "\n",
    "    return images, cls, np_utils.to_categorical(cls, nb_classes)\n",
    "X_train, cls_train, y_train = load_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the testing dataset using similar method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n"
     ]
    }
   ],
   "source": [
    "def load_test_data():\n",
    "    \"\"\"\n",
    "    Load all the test-data for the CIFAR-10 data-set.\n",
    "\n",
    "    Returns the images, class-numbers and one-hot encoded class-labels.\n",
    "    \"\"\"\n",
    "\n",
    "    images, cls = load_data(file_name=\"test_batch\")\n",
    "\n",
    "    return images, cls, np_utils.to_categorical(cls, nb_classes)\n",
    "X_test, cls_test, y_test = load_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the size of the training and testing set to check if everything is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set:\t\t50000\n",
      "- Test-set:\t\t10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(X_train)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our dataset is ready! Let's build a model on Keras to do the classification.\n",
    "I use three convolutional layer followed by max-polling, one fully-connected(dense) layer and a softmax layer in my network. We will talk more about convolutional layer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "40000/40000 [==============================] - 235s - loss: 1.6083 - acc: 0.4170 - val_loss: 1.2983 - val_acc: 0.5361\n",
      "Epoch 2/50\n",
      "40000/40000 [==============================] - 226s - loss: 1.1706 - acc: 0.5850 - val_loss: 1.0824 - val_acc: 0.6162\n",
      "Epoch 3/50\n",
      "40000/40000 [==============================] - 231s - loss: 0.9372 - acc: 0.6690 - val_loss: 0.9483 - val_acc: 0.6703\n",
      "Epoch 4/50\n",
      "40000/40000 [==============================] - 227s - loss: 0.7499 - acc: 0.7375 - val_loss: 0.9094 - val_acc: 0.6900\n",
      "Epoch 5/50\n",
      "40000/40000 [==============================] - 230s - loss: 0.5893 - acc: 0.7928 - val_loss: 0.9481 - val_acc: 0.7014\n",
      "Epoch 6/50\n",
      "40000/40000 [==============================] - 226s - loss: 0.4460 - acc: 0.8442 - val_loss: 1.0390 - val_acc: 0.6900\n",
      "Epoch 7/50\n",
      "40000/40000 [==============================] - 228s - loss: 0.3469 - acc: 0.8787 - val_loss: 1.0985 - val_acc: 0.6984\n",
      "Epoch 8/50\n",
      "40000/40000 [==============================] - 228s - loss: 0.2754 - acc: 0.9033 - val_loss: 1.2098 - val_acc: 0.6937\n",
      "Epoch 9/50\n",
      "40000/40000 [==============================] - 212s - loss: 0.2306 - acc: 0.9210 - val_loss: 1.3558 - val_acc: 0.6886\n",
      "Epoch 10/50\n",
      "40000/40000 [==============================] - 210s - loss: 0.2037 - acc: 0.9320 - val_loss: 1.3619 - val_acc: 0.7012\n",
      "Epoch 11/50\n",
      "40000/40000 [==============================] - 210s - loss: 0.1958 - acc: 0.9338 - val_loss: 1.4511 - val_acc: 0.6961\n",
      "Epoch 12/50\n",
      "40000/40000 [==============================] - 244s - loss: 0.1679 - acc: 0.9448 - val_loss: 1.5001 - val_acc: 0.6938\n",
      "Epoch 13/50\n",
      "40000/40000 [==============================] - 243s - loss: 0.1681 - acc: 0.9457 - val_loss: 1.6901 - val_acc: 0.6864\n",
      "Epoch 14/50\n",
      "40000/40000 [==============================] - 227s - loss: 0.1692 - acc: 0.9444 - val_loss: 1.7630 - val_acc: 0.6907\n",
      "Epoch 15/50\n",
      "39840/40000 [============================>.] - ETA: 1s - loss: 0.1479 - acc: 0.9524"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GaussianNoise, MaxoutDense\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# here are some settings for my network\n",
    "batch_size = 32\n",
    "nb_epoch = 50\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same',\n",
    "                        input_shape=(img_channels, img_size, img_size)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# let's train the model using SGD + momentum\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "%time his = model.fit(X_train, y_train, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss = his.history['loss']\n",
    "val_loss = his.history['val_loss']\n",
    "\n",
    "# visualize training history\n",
    "plt.plot(range(1, len(train_loss)+1), train_loss, color='blue', label='Train loss')\n",
    "plt.plot(range(1, len(val_loss)+1), val_loss, color='red', label='Val loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('./output/fig-nn-val-baseline.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently we are having the issues of over fitting here. It's a chance for us to learn some regularizations technique here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "Dropout is designed by Geoffrey Hinton.\n",
    "The key idea is to randomly drop some units from the neural network during training, so that the neuron have to function well on its own instead of relying on other neurons. (Just like when you know your teammates is not that reliable, you have to take more responsibility)\n",
    "In a standard neural network, the derivative received by each parameter tells it how it should change so the final loss function is reduced, given what all other units are doing. Therefore, units may change in a way that they fix up the mistakes of the other units. This may lead to complex co-adaptations.\n",
    "Applying dropout to a neural network amounts to sampling a “thinned” network from it. The thinned network consists of all the units that are not dropped out (Figure b). A neural net with $n$ units, can be seen as a collection of  possible thinned neural networks. These networks all share weights so that the total number of parameters is still $O(n^{2})$, or less. For each presentation of each training case, a new thinned network is sampled and trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figures illustrate the architecture:\n",
    "\n",
    "<img src=\"fig-dropout.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of which units to drop is random. In the simplest case, each unit is retained with a fixed probability $p$ independent of other units, where $p$ can be chosen using a validation set or can simply be set at 0.5, which seems to be close to optimal for a wide range of networks and tasks.\n",
    "At test time, it is not feasible to explicitly average the predictions from exponentially many thinned models. A very simple approximate averaging method works well in practice. The idea is to use a single neural net at test time **without dropout**. If a unit is retained with probability $p$ during training, the outgoing weights of that unit are multiplied by $p$ at test time as shown in the figure below. This is to ensure that for any hidden unit the expected output (under the distribution used to drop units at training time) is the same as the actual output at test time.\n",
    "<img src=\"fig-weight.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "40000/40000 [==============================] - 345s - loss: 1.7790 - acc: 0.3420 - val_loss: 1.5292 - val_acc: 0.4622\n",
      "CPU times: user 9min, sys: 26.2 s, total: 9min 27s\n",
      "Wall time: 5min 56s\n",
      "10000/10000 [==============================] - 33s    \n",
      "\n",
      "Test loss: 1.533\n",
      "Test accuracy: 0.459\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same',\n",
    "                        input_shape=(img_channels, img_size, img_size)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "# Dropout layer with p = 0.25\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "# add dropout\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "# add dropout\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "%time his = model.fit(X_train, y_train, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot_history(his)\n",
    "train_loss = his.history['loss']\n",
    "val_loss = his.history['val_loss']\n",
    "\n",
    "# visualize training history\n",
    "plt.plot(range(1, len(train_loss)+1), train_loss, color='blue', label='Train loss')\n",
    "plt.plot(range(1, len(val_loss)+1), val_loss, color='red', label='Val loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('#Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('./output/fig-nn-val-baseline.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We can see the dropout has a signifanct effect on the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxout\n",
    "Maxout is called maxout because its output is the max of a set of inputs. It wasa designed by Goodfellow on January 2013. \n",
    "You can simply create a dense layer with maxout by calling the MaxoutDense layer in Keras.\n",
    "A MaxoutDense layer takes the element-wise maximum of nb_feature Dense(input_dim, output_dim) linear layers. This allows the layer to learn a convex, piecewise linear activation function over the inputs.\n",
    "Given an input $v\\subseteq\\mathbb{R}^{d}$, a maxout hidden layer implements the function $h_{i}(x)=\\underset{j\\subseteq[1,k]}{max}z_{ij}$, where $z_{ij}=x^{T}W_{\\text{···}ij}+b_{ij}$, and $W\\subseteq\\mathbb{R}^{d\\times m\\times k}$and $b\\subseteq\\mathbb{R}^{m\\times k}$\n",
    "<img src=\"fig-maxout.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Decay\n",
    "Remember we talked about weight decay before? We can penalize large weights using penalties or constraints on their squared values (L2 penalty) or absolute values (L1 penalty).\n",
    "We specify in l1/l2 regularities by passing a regularizer to the layer.\n",
    "```\n",
    "from keras.regularizers import l1, l2 \n",
    "model.add(Dense(64, input_dim=64, W_regularizer=l2(0.01)))\n",
    "```\n",
    "\n",
    "## L1 weight cost\n",
    "\n",
    "l1 regularizer will result in a lot of zeros in the weight.\n",
    "\n",
    "\n",
    "## L2 weight cost\n",
    "It makes a smoother model in which the output changes more slowly as the input changes.\n",
    "If the network has two very similar inputs it prefers to put half the weight on each rather than all the weight on one.\n",
    "We illustrate it one the following figure.\n",
    "<img src=\"fig-smooth.png\" width=\"400\">\n",
    "This can often improve generalization a lot because it helps to stop the network from fitting the sampling errorm and it makes a smoother model in which the output changes more slowly as the input changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Noise\n",
    "We can add noise to input to prevent over-fitting. Previous work has shown that such training with noise is equivalent to a form of regularization in which an extra term is added to the error function.\n",
    "in keras, you can do so by calling\n",
    "```\n",
    "keras.layers.noise.GaussianNoise(sigma)\n",
    "```\n",
    "This will apply to the input an additive zero-centered Gaussian noise with standard deviation sigma. This is useful to avoid overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
